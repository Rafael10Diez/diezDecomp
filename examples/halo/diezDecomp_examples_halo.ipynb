{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halo Exchange Example\n",
    "This Jupyter notebook presents an example about performing halo exchanges in diezDecomp.\n",
    "\n",
    "Since halo exchanges are relatively simple operations, this example corresponds to a general case, where multiple variables are randomized:\n",
    "* Halo exchange direction (`ii`) in the global $x/y/z$ coordinate system ($x \\to 0,~y \\to 1,~z \\to 2$).\n",
    "* Number of halo cells in every direction (`nh_xyz`).\n",
    "* Array padding for each MPI task (`offset6`).\n",
    "* 3D array size (`n3`).\n",
    "* Periodicity in the halo exchange direction (`periodicity_xyz`)\n",
    "* Custom order for the local data (`order_halo`) (e.g., `x-y-z` or `y-z-x`).\n",
    "* Grid layout for the pencil distribution of the MPI tasks.\n",
    "\n",
    "To perform an exhaustive study, a Python script (`gen_random_halos.py`) generates random combinations of the previous variables, and the Fortran code is called. The results are verified by pre-computing the expected values after the halo exchange (`A_ref`), and checking with the results after the operation.\n",
    "\n",
    "In this example, most of the Fortran code is about defining the analytical values expected for `A_ref`. Only 3 lines of code are required for calling diezDecomp:\n",
    "\n",
    "```\n",
    "call diezdecomp_track_mpi_decomp(lo_ref, obj_ranks, irank, nproc)\n",
    "call diezdecomp_generic_fill_hl_obj(hl, obj_ranks, A_shape, offset6, ii, nh_xyz, order_halo, periodic_xyz, wsize, use_halo_sync, autotuned_pack)\n",
    "call diezdecomp_halos_execute_generic(hl, A, work)\n",
    "```\n",
    "\n",
    "The first subroutine (`diezdecomp_track_mpi_decomp`) is used to define the grid layout for the MPI tasks, which is stored in the object `obj_ranks`. Then, the descriptor for the halo exchange (`hl`) is initialized using the subroutine `diezdecomp_generic_fill_hl_obj`. Finally, during the CFD iterations, the subroutine `diezdecomp_halos_execute_generic` performs the halo exchange using the object descriptor `hl`, the input array `A`, and a small work buffer `work`.\n",
    "\n",
    "### Summary API Input\n",
    "A summary about all input variables for the diezDecomp subroutines in this example is given below:\n",
    "* `nproc` is the total number of MPI processes.\n",
    "* `irank` is the global rank of each MPI task.\n",
    "* `lo_ref(0:2)` is a reference `[i,j,k]` position for each MPI task in the global 1D/2D/3D pencil distribution for the simulation domain.\n",
    "   * Based on the `lo_ref` coordinates, diezDecomp automatically tracks the location of each MPI task in the physical domain. \n",
    "   * Any reference coordinate can be used as `lo_ref(0:2)`, as long as it properly describes the location of each MPI task in the 1D/2D/3D pencil distribution.\n",
    "       * However, please note that all MPI tasks aligned along a Cartesian direction ($x/y/z$) are required to have the same coordinate (e.g. `lo_ref(0)=i=0` for the first slice along the $x$-direction).\n",
    "* `ii` is the global direction of the halo exchange operation: ($x \\to 0,~y \\to 1,~z \\to 2$).\n",
    "* `nh_xyz` is the number of halo cells along the $x/y/z$ directions.\n",
    "    * While the current halo exchange operation is along the `ii`-direction, DNS/LES solvers will typically have arrays with reserved halo cells along multiple directions at once.\n",
    "* `periodic_xyz` is the presence of periodic boundary conditions along the $x/y/z$ directions.\n",
    "    * Please note that only the periodicity along the `ii`-direction will be considered for the halo exchange operation.\n",
    "    * Other periodic boundary conditions might be used for subsequent halo exchanges.\n",
    "* `order_halo` is the local order for the input array `A`. \n",
    "    * For instance, `order_halo=[1,2,0]` implies that the array `A` has an indexing system ordered in the $y/z/x$ directions.\n",
    "* `offset6` is the (ignored) padding for the input array `A` in each dimension:\n",
    "    * `offset6(i,:,0)` is the padding at the beggining of the dimension `i`, whereas `offset6_in(i,:,1)` is the padding added at the end of such dimension.\n",
    "    * This padding is external to the reserved cells for the halo exchange (`nh_xyz`):\n",
    "        * The cells described by `offset6` are ignored by the halo exchange system.\n",
    "        * DNS/LES solvers can require extra padded cells (`offset6`) for various practical reasons, such as re-using an allocated array for another task.\n",
    "    * Please note that `offset6` follows the order specified by `order_halo`.\n",
    "* `A_shape` is the shape of the input array `A`, including the (ignored) padded cells `offset6` and the reserved halo cells `nh_xyz`.\n",
    "    * The order of `A_shape` is consistent with `order_halo`.\n",
    "\n",
    "* `use_halo_sync` is a Boolean flag indicating if synchronous halo exchange operations are needed.\n",
    "    * Most applications work faster with `use_halo_sync = .false.`, but the option is still available. The performance differences are minor.\n",
    "* `autotuned_pack` is a Boolean flag to auto-tune the data packing algorithm for the halo exchange.\n",
    "    * Usually, data packing operations in halo exchanges are very fast compared to MPI transfers, and thus this auto-tuning feature can be disabled.\n",
    "\n",
    "Additional notes:\n",
    "* The variable `wsize` in `diezdecomp_generic_fill_hl_obj` is a secondary output, indicating the minimum size of the work buffer (`work`) needed by the halo exchange operation.\n",
    "    * The user is responsible for ensuring that the size of the `work` array is larger than `wsize`.\n",
    "    * If multiple halo exchanges are performed, `diezdecomp_generic_fill_hl_obj` can be called during initilization (without allocated arrays) to identify the maximum value of `wsize`.\n",
    "* In the Fortran input files, the variable `flat_mpi_ranks` corresponds to a special array, where `flat_mpi_ranks(rank,:)` indicates the global `[i,j,k]` position of the MPI task `rank` within the global pencil distribution.\n",
    "    * If such array is available, it is one of the best choices to define `lo_ref(0:2)` for the subroutine `diezdecomp_generic_fill_hl_obj`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotuning for Halo Exchanges\n",
    "\n",
    "In diezDecomp, two autotuning options are available for halo exchanges:\n",
    "* MPI data transfer mode (synchronous vs. asynchronous).\n",
    "* Data packing/unpacking algorithm (batched vs. simultaneous).\n",
    "\n",
    "Each option is described separately below:\n",
    "\n",
    "### MPI data transfer mode\n",
    "\n",
    "In diezDecomp, halo exchanges can be performed using either asynchronous (`MPI_ISend/IRecv`) MPI operations for data transfer, or synchronous (`MPI_SendRecv`) pairs. Usually, asynchronous transfers are slightly faster than synchronous operations, yet both options are available for autotuning.\n",
    "\n",
    "The `integer` parameters to control MPI transfers are:\n",
    "* CaNS API: global variable `diezdecomp_halo_mpi_mode`.\n",
    "* Generic API: input variable `force_halo_sync` for the subroutine `diezdecomp_generic_fill_hl_obj`.\n",
    "\n",
    "For both variables (`diezdecomp_halo_mpi_mode` and `force_halo_sync`), the following conventions are used:\n",
    "* `1`: Synchronous MPI operations (`MPI_SendRecv`).\n",
    "* `2`: Asynchronous MPI transfers (`MPI_ISend/IRecv`).\n",
    "* Other values: enable autotuning.\n",
    "\n",
    "By default, the CaNS API is configured to use asynchronous transfers (`MPI_ISend/IRecv`). Autotuning must be enabled manually by changing `diezdecomp_halo_mpi_mode`.\n",
    "\n",
    "### Data packing/unpacking algorithm\n",
    "\n",
    "In halo exchanges, the main performance bottleneck are usually MPI data transfers. However, performance improvements can also be found by optimizing other operations. For example, MPI transfers require 1D data buffers, containing all information sent or received by the operation. Packing and unpacking data from these buffers requires especialized GPU kernels. \n",
    "\n",
    "In diezDecomp, two options are available for data packing/unpacking from GPU kernels.\n",
    "* \"Batched\" mode:\n",
    "    * Separate GPU kernels are launched to pack/unpack data from every slice in the input array (`A(i,j,k)`) participating in the halo exchange.\n",
    "* \"Simultaneous\" mode:\n",
    "    * Only one GPU kernel is launched to pack/unpack data from the entire MPI 1D buffer.\n",
    "\n",
    "Both of the previous options are available for autotuning (\"batched\" vs. \"simultaneous\"). Generally speaking, the \"batched\" mode has GPU kernels with shorter instructions, and it is (slightly) faster for halo exchanges requiring few cells (e.g. `nh_xyz = [2,2,2]`). The \"simultaneous\" mode has GPU kernels with generalized code, and it tends to be faster for halo exchanges requiring many cells (e.g. `nh_xyz >> [2,2,2]`).\n",
    "\n",
    "The variables for controlling the packing/unpacking behavior of the halo exchange are<sup>1</sup>:\n",
    "* CaNS API: global variable `diezdecomp_halo_autotuned_pack`.\n",
    "* Generic API: input variable `autotuned_pack` in the subroutine `diezdecomp_generic_fill_hl_obj`.\n",
    "\n",
    "The conventions for the variables `diezdecomp_halo_autotuned_pack` and `autotuned_pack` are:\n",
    "* `2`: \"batched\" mode.\n",
    "* `3`: \"simultaneous\" mode.\n",
    "* Other values: enable autotuning.\n",
    "\n",
    "By default, the CaNS API enables the \"batched\" mode (`2`), because it tends to be faster for the halo exchanges found in the CaNS project. However, please note that the \"simultaneous\" mode (`3`) could be faster for DNS/LES solvers using high-order methods requiriring large halo exchanges.\n",
    "\n",
    "<sup>1</sup>: The numbers `2` and `3` are a reference to the \"batched\" mode (`2`) working with 2D slices, and the \"simultaneous\" mode (`3`) processing entire 3D arrays.\n",
    "\n",
    "### Autotuning Report (Halo Exchanges)\n",
    "\n",
    "In diezDecomp, all benchmark results for autotuning operations (with halo exchanges) are recorded inside the object (`hl`). A summary of the results can be printed with the subroutine `diezdecomp_summary_halo_autotuning`, without performing any additional benchmarks. This allows the user to better understand how different autotuning choices influence the speed of halo exchanges, and develop practical guidelines for manually choosing parameters.\n",
    "\n",
    "\n",
    "Due to the limited number of autotuning parameters for halo exchanges, please note that most DNS/LES solvers can operate with fixed choices delivering high performance:\n",
    "* Synchronous vs asynchronous MPI transfers.\n",
    "* Batched vs. simultaneous data packing/unpacking modes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Information\n",
    "\n",
    "After analyzing the previous examples, further details about diezDecomp halo exchanges can be found in the specialized test suite: `./tests/halo`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
